"""
CodeScoreç±»ç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆçš„è´¨é‡ã€‚

è¿™ä¸ªç±»ä½¿ç”¨OpenAI APIï¼ˆæˆ–å…¼å®¹çš„APIï¼‰æ¥è¯„ä¼°ç”Ÿæˆçš„ä»£ç ä¸è®ºæ–‡æè¿°çš„åŒ¹é…ç¨‹åº¦ã€‚
å®ƒæ”¯æŒä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼šæ— å‚è€ƒè¯„ä¼°(ref_free)å’ŒåŸºäºå‚è€ƒçš„è¯„ä¼°(ref_based)ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è¯»å–è®ºæ–‡å†…å®¹å’Œç”Ÿæˆçš„ä»£ç 
2. ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„åˆ†
3. æ”¶é›†å¤šä¸ªè¯„ä¼°ç»“æœå¹¶è®¡ç®—å¹³å‡åˆ†æ•°
4. ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœå’Œç†ç”±

ä½¿ç”¨ç¤ºä¾‹ï¼š
    codescore = CodeScore(eval_type="ref_free")
    result = codescore.score(
        strategy_name="Transformer",
        pdf_json_path="./examples/Transformer_cleaned.json",
        target_repo_dir="./outputs/Transformer_repo",
        gold_repo_dir=""
    )
"""

import json
from openai import OpenAI
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from codes.utils import read_all_files,num_tokens_from_messages,extract_json_from_string,get_now_str,print_log_cost
from tqdm import tqdm

# è®¾ç½®APIé…ç½®
os.environ["BASE_URL"] = "https://api.siliconflow.cn/v1"
with open("./api_key/SiliconCloud.api") as f:
    os.environ["OPENAI_API_KEY"] = f.readline()

class CodeScore():
    """
    ä»£ç è¯„åˆ†ç±»ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆçš„ä»£ç è´¨é‡ã€‚

    å‚æ•°:
        generated_n (int): ç”Ÿæˆè¯„ä¼°ç»“æœçš„æ¬¡æ•°ï¼Œé»˜è®¤ä¸º5
        model (str): ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º"Qwen/Qwen3-8B"
        eval_type (str): è¯„ä¼°ç±»å‹ï¼Œå¯é€‰"ref_free"æˆ–"ref_based"ï¼Œé»˜è®¤ä¸º"ref_based"
        eval_result_dir (str): è¯„ä¼°ç»“æœä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º"score"
    """
    def __init__(self,generated_n=5,model="Qwen/Qwen3-8B",eval_type="ref_based",eval_result_dir="score") -> None:
        self.client = OpenAI(base_url=os.environ["BASE_URL"],api_key = os.environ["OPENAI_API_KEY"])
        self.eval_type = eval_type
        self.generated_n = generated_n
        self.model = model
        self.eval_result_dir = eval_result_dir

    def api_call(self,request_json):
        """
        è°ƒç”¨APIè¿›è¡Œè¯„åˆ†ã€‚

        å‚æ•°:
            request_json (dict): APIè¯·æ±‚å‚æ•°

        è¿”å›:
            completion: APIè¿”å›çš„å®Œæˆç»“æœ
        """
        completion = self.client.chat.completions.create(**request_json)
        return completion

    def read_strategy(self,pdf_json_path):
        """
        è¯»å–è®ºæ–‡ç­–ç•¥æ–‡ä»¶ã€‚

        å‚æ•°:
            pdf_json_path (str): è®ºæ–‡JSONæ–‡ä»¶è·¯å¾„

        è¿”å›:
            dict: è®ºæ–‡å†…å®¹
        """
        with open(f'{pdf_json_path}') as f:
            paper_json = json.load(f)
        return paper_json

    def read_code(self,target_repo_dir,allowed_ext=[".py", ".yaml", ".yml", ".md", ".sh", ".bash"],is_print=False):
        """
        è¯»å–ä»£ç æ–‡ä»¶ã€‚

        å‚æ•°:
            target_repo_dir (str): ç›®æ ‡ä»£ç ä»“åº“ç›®å½•
            allowed_ext (list): å…è®¸çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
            is_print (bool): æ˜¯å¦æ‰“å°æ–‡ä»¶ä¿¡æ¯

        è¿”å›:
            str: æ ¼å¼åŒ–åçš„ä»£ç å­—ç¬¦ä¸²
        """
        target_files_dict = read_all_files(target_repo_dir, allowed_ext=allowed_ext, is_print=is_print)
        codes = ""
        for file_name, code in target_files_dict.items():
            codes += f"```## File name: {file_name}\n{code}\n```\n\n" 
        return codes

    def score(self,strategy_name,
              paper_json=None,codes=None,goldcodes=None
              ,pdf_json_path=None,target_repo_dir=None,gold_repo_dir=None):
        """
        æ‰§è¡Œä»£ç è¯„åˆ†ã€‚

        å‚æ•°:
            strategy_name (str): ç­–ç•¥åç§°
            paper_json (dict, optional): è®ºæ–‡å†…å®¹
            codes (str, optional): ç”Ÿæˆçš„ä»£ç 
            goldcodes (str, optional): å‚è€ƒä»£ç 
            pdf_json_path (str, optional): è®ºæ–‡JSONæ–‡ä»¶è·¯å¾„
            target_repo_dir (str, optional): ç›®æ ‡ä»£ç ä»“åº“ç›®å½•
            gold_repo_dir (str, optional): å‚è€ƒä»£ç ä»“åº“ç›®å½•

        è¿”å›:
            dict: è¯„ä¼°ç»“æœï¼ŒåŒ…å«åˆ†æ•°å’Œè¯„ä¼°ç†ç”±
        """
        # è¯»å–è¯„ä¼°æç¤ºè¯
        prompt = open(f"./prompts/score/{self.eval_type}.txt").read()
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        if paper_json is None:
            paper_json = self.read_strategy(pdf_json_path)
        if codes is None:
            codes = self.read_code(target_repo_dir)
        cur_prompt = prompt.replace('{{Paper}}', f"{paper_json}").replace('{{Code}}', codes)
        if goldcodes is None:
            goldcodes = self.read_code(gold_repo_dir)
        cur_prompt = cur_prompt.replace('{{GoldCode}}', f"{goldcodes}")
        msg = [{"role": "system", "content": cur_prompt}]

        # æ£€æŸ¥tokenæ•°é‡
        try:
            num_tokens = num_tokens_from_messages(msg)
        except Exception as e:
            print(f"[WARNING] An exception was raised while counting tokens for the target repository of {strategy_name}.")
            print(e)
            print("-"*40)
            num_tokens = 0
        assert num_tokens <= 128000

        # è¯„åˆ†é”®å
        score_key = "score"
        rationale_key = "critique_list"

        # æ”¶é›†è¯„åˆ†ç»“æœ
        all_scores = []
        rationales = []
        for n in tqdm(range(self.generated_n),desc=f"Repeat scoring"):    
            request_json = {
                    "model": self.model, 
                    "messages": msg, 
                    "temperature": 1,
                    "frequency_penalty": 0,
                    "presence_penalty": 0,
                    "stop": None
            }
            completion = self.api_call(request_json)
            completion_json = json.loads(completion.model_dump_json())
            choice = completion_json['choices'][0]
            output = choice['message']['content'].strip()
            
            # è§£æè¯„åˆ†ç»“æœ
            try:
                output_json2 = json.loads(output)
                score = int(output_json2[score_key])

                if isinstance(output_json2[rationale_key], str):
                    rationale = output_json2[rationale_key]
                else:
                    rationale = json.dumps(output_json2[rationale_key])
            except Exception as e:
                try:
                    output_json2 = json.loads(extract_json_from_string(output))
                    score = int(output_json2[score_key])

                    if isinstance(output_json2[rationale_key], str):
                        rationale = output_json2[rationale_key]
                    else:
                        rationale = json.dumps(output_json2[rationale_key])
                except Exception as e2:
                    print(f"[WARNING] Invalid repsponse: parsing error")
                    print(e2)
                    print("-"*40)
                    continue
                
            # éªŒè¯åˆ†æ•°èŒƒå›´
            if score < 1 or score > 5:
                print(f"[WARNING] Invalid repsponse: score {score}, Score must be in the range of 1â€“5.")
                continue
            
            all_scores.append(int(score))
            rationales.append(rationale)

        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_score = sum(all_scores) / len(all_scores)

        # æ„å»ºè¾“å‡ºç»“æœ
        output_json= {
            "strategy_name": strategy_name,
            "target_repo_dir": target_repo_dir,
            "eval_type": self.eval_type,
            "gold_repo_dir": gold_repo_dir,
            "generated_n": self.generated_n,
            "request_json": request_json,
            "completion_json": completion_json,
            "eval_result": {
                "score": avg_score,
                "valid_n": len(all_scores),
                "scroe_lst": all_scores,
                "rationale_lst": rationales,    
            },
        }
        
        # ä¿å­˜ç»“æœ
        now_str = get_now_str()
        output_dir = os.path.join(self.eval_result_dir,f"{strategy_name}_eval_{self.eval_type}_{self.model}_{now_str}")
        os.makedirs(output_dir, exist_ok=True)
        with open(os.path.join(output_dir, "score.json"), 'w', encoding='utf-8') as f:
            json.dump(output_json, f)

        # æ‰“å°è¯„ä¼°æ‘˜è¦
        print()
        print("=" * 40)
        print("ğŸŒŸ Evaluation Summary ğŸŒŸ")
        print(f"ğŸ“„ Strategy name: {strategy_name}")
        print(f"ï¿½ï¿½ Evaluation type: {self.eval_type}")
        print(f"ğŸ“ Target repo directory: {target_repo_dir}")
        print(f"ğŸ“Š Evaluation result:")
        print(f"\tğŸ“ˆ Score: {avg_score:.4f}")
        print(f"\tâœ… Valid: {output_json['eval_result']['valid_n']}/{self.generated_n}")
        print("=" * 40)
        
        print_log_cost(completion_json, self.model, f"[Evaluation] {strategy_name} - {self.eval_type}", output_dir, 0)
        return output_json

if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    codescore = CodeScore(eval_type="ref_free")
    codescore.score(strategy_name="Transformer",pdf_json_path="./examples/Transformer_cleaned.json",target_repo_dir="./outputs/Transformer_repo",gold_repo_dir="")
    print("end")